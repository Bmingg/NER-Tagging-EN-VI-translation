{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc6cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as p\n",
    "from wtpsplit import SaT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d05df",
   "metadata": {},
   "source": [
    "**This notebook goes through the flow of processing en and vi text data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4b319",
   "metadata": {},
   "source": [
    "# Segment text data into sentences, for translation to vietnamese of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the json data file\n",
    "json_path = \"../data/json/20250509_corpus_pubtator_output.json\"\n",
    "f = open(json_path)\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SaT object for sentence segmentation.\n",
    "sat_sm = SaT(\"sat-12l-sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e30cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each sentence in each data point by \"\\n\"\n",
    "\n",
    "for (i, data_point) in enumerate(data):\n",
    "    print(i)\n",
    "    dummy = data[i][\"data\"][\"text\"]\n",
    "\n",
    "    en_sents = sat_sm.split(dummy, strip_whitespace=True)\n",
    "    en_sents = \"\\n\".join(en_sents)\n",
    "\n",
    "    data[i][\"data\"][\"text\"] = en_sents\n",
    "\n",
    "with open(\"../data/json/sentence_pubtator_output.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047d0e9",
   "metadata": {},
   "source": [
    "# Translation. Do not run since translation has been done. Model: vinai/vinai-translate-en2vi-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c102c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# 1. Load the tokenizer and model for enâ†’vi translation\n",
    "tokenizer_en2vi = AutoTokenizer.from_pretrained(\n",
    "    \"vinai/vinai-translate-en2vi-v2\",\n",
    "    src_lang=\"en_XX\"\n",
    ")\n",
    "model_en2vi = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"vinai/vinai-translate-en2vi-v2\"\n",
    ")\n",
    "\n",
    "# using CPU is not recommended\n",
    "device_en2vi = torch.device(\"cuda\")\n",
    "model_en2vi.to(device_en2vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_en2vi(en_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Translate a single English sentence into Vietnamese.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sentence, with padding/truncation\n",
    "    inputs = tokenizer_en2vi(\n",
    "        en_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    # This part can be removed since we are translating sentence by sentence.\n",
    "    # Thus, the input length will not exceed the max length.\n",
    "    if len(inputs['input_ids'][0]) > 1024:\n",
    "        sents = en_text.split(\". \")\n",
    "        text1 = \". \".join(sents[:5]) + \".\"\n",
    "        text2 = \". \".join(sents[5:]) + \".\"\n",
    "        vi_text1 = translate_en2vi(text1)\n",
    "        vi_text2 = translate_en2vi(text2)\n",
    "        vi_text = vi_text1 + \" \" + vi_text2\n",
    "    else:\n",
    "        # Generate translation with beam search\n",
    "        output_ids = model_en2vi.generate(\n",
    "            inputs.input_ids.to(device_en2vi),\n",
    "            decoder_start_token_id=tokenizer_en2vi.lang_code_to_id[\"vi_VN\"],\n",
    "            num_return_sequences=1,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        # Decode the generated IDs back to text\n",
    "        vi_text = tokenizer_en2vi.batch_decode(\n",
    "            output_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "    return vi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e17592",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/json/sentence_pubtator_output.json\"\n",
    "f = open(path)\n",
    "data = json.load(f)\n",
    "# Sample: translating the first 100 sentences\n",
    "translate = data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(translate)\n",
    "\n",
    "for (i, data_point) in enumerate(translate):\n",
    "    print((i + 1) / total * 100)\n",
    "    preds = []\n",
    "    en_text = translate[i][\"data\"][\"text\"]\n",
    "    en_sents = en_text.split(\"\\n\")\n",
    "    for en_sent in en_sents:\n",
    "        vi_text_pred = translate_en2vi(en_sent)\n",
    "        preds.append(vi_text_pred)\n",
    "\n",
    "    vi_text = \"\\n\".join(preds)\n",
    "    translate[i][\"data\"][\"vi_text\"] = vi_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current: 0 -> 99\n",
    "\n",
    "out_path = \"../data/json/translation0-99.json\"\n",
    "with open(out_path, \"w\") as outfile:\n",
    "    json.dump(translate, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06164c88",
   "metadata": {},
   "source": [
    "# Replacing \"\\n\" with \"\\n\\n\" for better UI in label studio and re-label the tagging indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02e925",
   "metadata": {},
   "source": [
    "**Tips for reproducing the result:**\n",
    "\n",
    "3 datapoints that respectively contain these keywords: \"Differential transcriptome expression in human nucleus\", \"Patient - Physician Discordance in Global Assessment\", \"Bioinformatic analysis of RNA-seq data\" were manually translated afterward.\n",
    "\n",
    "For reproduction, start from the beginning of this notebook, and redo the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6135ea76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4392"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and concat all translation data\n",
    "json_path0 = \"../data/json/translation0-99.json\"\n",
    "json_path1 = \"../data/json/translation100-2999.json\"\n",
    "json_path2 = \"../data/json/translation3000-all.json\"\n",
    "f0 = open(json_path0)\n",
    "f1 = open(json_path1)\n",
    "f2 = open(json_path2)\n",
    "data0 = json.load(f0)\n",
    "data1 = json.load(f1)\n",
    "data2 = json.load(f2)\n",
    "data = data0 + data1 + data2\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20abd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change field name from \"text\" to \"english_text\"\n",
    "for (i, data_point) in enumerate(data):\n",
    "    data[i][\"data\"][\"en_text\"] =  data[i][\"data\"].pop(\"text\")\n",
    "    for (j, labeling) in enumerate(data_point[\"predictions\"][0]['result']):\n",
    "        data[i][\"predictions\"][0]['result'][j]['from_name'] = \"en_label\"\n",
    "        data[i][\"predictions\"][0]['result'][j]['to_name'] = \"en_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f310219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for length mismatch between the number en and vi sentences in each data point\n",
    "for (i, data_point) in enumerate(data):\n",
    "    en_text = data[i][\"data\"][\"en_text\"]\n",
    "    en_sents = en_text.split(\"\\n\")\n",
    "    vi_text = data[i][\"data\"][\"vi_text\"]\n",
    "    vi_sents = vi_text.split(\"\\n\")\n",
    "    if len(en_sents) != len(vi_sents):\n",
    "        print(\"Length mismatch\")\n",
    "        print(f\"en: {len(en_sents)}\")\n",
    "        print(f\"vi: {len(vi_sents)}\")\n",
    "        print(f\"i: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aad531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo to update the start and end index of each labeling\n",
    "for (i, data_point) in enumerate(data):\n",
    "    dummy = data[i][\"data\"][\"en_text\"]\n",
    "\n",
    "    for (j, labeling) in enumerate(data_point[\"predictions\"][0]['result']):\n",
    "        start_index = labeling['value']['start']\n",
    "        temp = dummy[:start_index]\n",
    "        c = temp.count(\".\\n\")\n",
    "        # c = temp.count(\". \")\n",
    "        data[i][\"predictions\"][0]['result'][j]['value']['start'] += c\n",
    "        data[i][\"predictions\"][0]['result'][j]['value']['end'] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5de547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace .\\n with .\\n\\n\n",
    "for (i, data_point) in enumerate(data):\n",
    "    data[i][\"data\"][\"en_text\"] = data[i][\"data\"][\"en_text\"].replace(\"\\n\", \"\\n\\n\")\n",
    "    data[i][\"data\"][\"vi_text\"] = data[i][\"data\"][\"vi_text\"].replace(\"\\n\", \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3992e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/vubinhminh/Work/Machine Translation/data/json/relabeled_corpus_pubtator_output.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef211af",
   "metadata": {},
   "source": [
    "# Formatting as an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63688e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/json/relabeled_corpus_pubtator_output.json\", encoding='utf-8')\n",
    "# f = open(\"../data/json/mapped_corpus_pubtator_output.json\", encoding='utf-8')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant data\n",
    "codes = []\n",
    "editor_names = []\n",
    "english = []\n",
    "vietnamese = []\n",
    "en_word_counts = []\n",
    "vi_word_counts = []\n",
    "edited_versions = []\n",
    "prof_chien_checking = []\n",
    "notes = []\n",
    "urls = []\n",
    "\n",
    "for (i, data_point) in enumerate(data):\n",
    "    # print(i+1)\n",
    "    en_text = data[i][\"data\"][\"en_text\"]\n",
    "    vi_text = data[i][\"data\"][\"vi_text\"]\n",
    "    en_sents = en_text.split(\"\\n\\n\")\n",
    "    vi_sents = vi_text.split(\"\\n\\n\")\n",
    "    english.extend(en_sents)\n",
    "    vietnamese.extend(vi_sents)\n",
    "    local_en_word_counts = [len(re.findall(r'\\b\\w+\\b', en_sent)) for en_sent in en_sents]\n",
    "    local_vi_word_counts = [len(re.findall(r'\\b\\w+\\b', vi_sent)) for vi_sent in vi_sents]\n",
    "    en_word_counts.extend(local_en_word_counts)\n",
    "    vi_word_counts.extend(local_vi_word_counts)\n",
    "    sents_len = len(en_sents)\n",
    "    if len(vi_sents) != len(en_sents):\n",
    "        print(f\"Error: {i}th data point has different number of sentences\")\n",
    "        print(f\"{len(en_sents)} English: {en_text}\")\n",
    "        print(f\"{len(vi_sents)} Vietnamese: {vi_text}\")\n",
    "        break\n",
    "    code = f\"item {i+1:04d}\"\n",
    "    local_codes = [code] * sents_len\n",
    "    codes.extend(local_codes)\n",
    "    dummy = [''] * sents_len\n",
    "    editor_names.extend(dummy)\n",
    "    edited_versions.extend(dummy)\n",
    "    prof_chien_checking.extend(dummy)\n",
    "    notes.extend(dummy)\n",
    "    urls.extend(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pd DataFrame\n",
    "df = pd.DataFrame([en_word_counts, vi_word_counts, editor_names, english, vietnamese, edited_versions, prof_chien_checking, notes, codes, urls]).T\n",
    "df.columns = columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pd DataFrame to an excel file\n",
    "xlsx_path = \"/Users/vubinhminh/Work/Machine Translation/data/xlsx/translated.xlsx\"\n",
    "with pd.ExcelWriter(xlsx_path, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, index=False)\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        \n",
    "        format_wrap = workbook.add_format({'text_wrap': True, 'valign': 'vcenter'})\n",
    "        \n",
    "        column_widths = [12, 12, 12, 50, 50, 50, 50, 20, 8, 20]\n",
    "        \n",
    "        for col_num, (col, width) in enumerate(zip(df.columns, column_widths)):\n",
    "            worksheet.set_column(col_num, col_num, width, format_wrap)\n",
    "        \n",
    "        for row_num in range(len(df) + 1):\n",
    "            worksheet.set_row(row_num, None, format_wrap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
